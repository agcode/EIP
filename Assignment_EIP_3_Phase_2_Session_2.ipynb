{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment EIP 3 Phase 2 Session 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agcode/EIP/blob/master/Assignment_EIP_3_Phase_2_Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpYQlwXzijeO",
        "colab_type": "text"
      },
      "source": [
        "# LSTM Exercise\n",
        "\n",
        "This code is reproduced with modifications from the [machine learning mastery](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/) blog."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DVhUJ-O5Xc-",
        "colab_type": "code",
        "outputId": "08403491-bd27-4d9e-e296-a41825506cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwmkaiOlFYhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMB2DhB0i4l4",
        "colab_type": "text"
      },
      "source": [
        "## The text for training\n",
        "\n",
        "the text is from [Alice's Adventures in Wonderland](http://www.gutenberg.org/cache/epub/11/pg11.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKjk5UfR4V-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file='/content/drive/My Drive/wonderland.txt'\n",
        "!cp \"$file\" .\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text_with_propercase = open(filename).read()\n",
        "raw_text_lower = raw_text_with_propercase.lower().split('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IPic4b9jGxe",
        "colab_type": "text"
      },
      "source": [
        "### Removed punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVfdE_WpWsBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_text=[]\n",
        "for j in raw_text_lower:\n",
        "  raw_text_char=''\n",
        "  for i in j:\n",
        "    if (ord(i)>32 and ord(i)<48) or (ord(i)>57 and ord(i)<97):\n",
        "      pass\n",
        "    else:\n",
        "      raw_text_char=raw_text_char+i\n",
        "  raw_text.append(raw_text_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN5o_zaVMTB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_text_master=''\n",
        "for i in raw_text:\n",
        "  raw_text_master=raw_text_master+i\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThSb8PBI493z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text_master)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Z-TguG_hO8",
        "colab_type": "code",
        "outputId": "f288ba33-5327-44a5-e3f4-f80219233db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "n_chars = len(raw_text_master)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  136089\n",
            "Total Vocab:  30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHmEeG_gjWHc",
        "colab_type": "text"
      },
      "source": [
        "### Existing dataset code without padding, considering whole text as one line "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycjiqaZCFGp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # prepare the dataset of input to output pairs encoded as integers\n",
        "# seq_length = 100\n",
        "# dataX = []\n",
        "# dataY = []\n",
        "# for i in range(0, n_chars - seq_length, 1):\n",
        "# \tseq_in = raw_text[i:i + seq_length]\n",
        "# \tseq_out = raw_text[i + seq_length]\n",
        "# \tdataX.append([char_to_int[char] for char in seq_in])\n",
        "# \tdataY.append(char_to_int[seq_out])\n",
        "# n_patterns = len(dataX)\n",
        "# print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPmAS-tgjfi-",
        "colab_type": "text"
      },
      "source": [
        "### New code that data set is created from  each line and padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_lG0P0_J27T",
        "colab_type": "code",
        "outputId": "991a2e1e-aa75-41b8-84b7-6de7905c0100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "seq_length = 15\n",
        "dataX = []\n",
        "dataY = []\n",
        "for line_string in raw_text:\n",
        "  for j in range(1,len(line_string)-1):\n",
        "    if j<=seq_length:\n",
        "      seq_in = pad_sequences([[char_to_int[char] for char in line_string[:j]]],maxlen=seq_length)\n",
        "    else:         \n",
        "      seq_in = pad_sequences([[char_to_int[char] for char in line_string[j-seq_length:]]],maxlen=seq_length,truncating='post', padding='post')\n",
        "    seq_out = char_to_int[line_string[j :j + 1]]\n",
        "    dataX.append(seq_in[0])\n",
        "    dataY.append(seq_out)\n",
        "#   break\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  134108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efaXvXEjn0o",
        "colab_type": "text"
      },
      "source": [
        "### Normalising Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGAayb885gWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yHC8Ml_jqVf",
        "colab_type": "text"
      },
      "source": [
        "### Model Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1JG41pT5SRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAfhkA9_jtcF",
        "colab_type": "text"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb_6iadhEDyT",
        "colab_type": "code",
        "outputId": "64b5abcd-88b1-4d49-8e53-7ca6e1cc28b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# define the checkpoint\n",
        "# filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "filename=\"lstm.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filename, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134108/134108 [==============================] - 47s 350us/step - loss: 2.7798\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.77978, saving model to lstm.hdf5\n",
            "Epoch 2/100\n",
            "134108/134108 [==============================] - 45s 336us/step - loss: 2.5091\n",
            "\n",
            "Epoch 00002: loss improved from 2.77978 to 2.50907, saving model to lstm.hdf5\n",
            "Epoch 3/100\n",
            "134108/134108 [==============================] - 45s 337us/step - loss: 2.3147\n",
            "\n",
            "Epoch 00003: loss improved from 2.50907 to 2.31473, saving model to lstm.hdf5\n",
            "Epoch 4/100\n",
            "134108/134108 [==============================] - 45s 334us/step - loss: 2.1832\n",
            "\n",
            "Epoch 00004: loss improved from 2.31473 to 2.18319, saving model to lstm.hdf5\n",
            "Epoch 5/100\n",
            "134108/134108 [==============================] - 45s 332us/step - loss: 2.0937\n",
            "\n",
            "Epoch 00005: loss improved from 2.18319 to 2.09373, saving model to lstm.hdf5\n",
            "Epoch 6/100\n",
            "134108/134108 [==============================] - 45s 335us/step - loss: 2.0248\n",
            "\n",
            "Epoch 00006: loss improved from 2.09373 to 2.02476, saving model to lstm.hdf5\n",
            "Epoch 7/100\n",
            "134108/134108 [==============================] - 45s 337us/step - loss: 1.9680\n",
            "\n",
            "Epoch 00007: loss improved from 2.02476 to 1.96799, saving model to lstm.hdf5\n",
            "Epoch 8/100\n",
            "134108/134108 [==============================] - 45s 338us/step - loss: 1.9217\n",
            "\n",
            "Epoch 00008: loss improved from 1.96799 to 1.92172, saving model to lstm.hdf5\n",
            "Epoch 9/100\n",
            "134108/134108 [==============================] - 45s 335us/step - loss: 1.8847\n",
            "\n",
            "Epoch 00009: loss improved from 1.92172 to 1.88470, saving model to lstm.hdf5\n",
            "Epoch 10/100\n",
            "134108/134108 [==============================] - 45s 334us/step - loss: 1.8506\n",
            "\n",
            "Epoch 00010: loss improved from 1.88470 to 1.85063, saving model to lstm.hdf5\n",
            "Epoch 11/100\n",
            "134108/134108 [==============================] - 45s 333us/step - loss: 1.8202\n",
            "\n",
            "Epoch 00011: loss improved from 1.85063 to 1.82022, saving model to lstm.hdf5\n",
            "Epoch 12/100\n",
            "134108/134108 [==============================] - 45s 333us/step - loss: 1.7941\n",
            "\n",
            "Epoch 00012: loss improved from 1.82022 to 1.79407, saving model to lstm.hdf5\n",
            "Epoch 13/100\n",
            "134108/134108 [==============================] - 45s 333us/step - loss: 1.7661\n",
            "\n",
            "Epoch 00013: loss improved from 1.79407 to 1.76614, saving model to lstm.hdf5\n",
            "Epoch 14/100\n",
            "134108/134108 [==============================] - 44s 332us/step - loss: 1.7448\n",
            "\n",
            "Epoch 00014: loss improved from 1.76614 to 1.74476, saving model to lstm.hdf5\n",
            "Epoch 15/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.7249\n",
            "\n",
            "Epoch 00015: loss improved from 1.74476 to 1.72491, saving model to lstm.hdf5\n",
            "Epoch 16/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.7077\n",
            "\n",
            "Epoch 00016: loss improved from 1.72491 to 1.70766, saving model to lstm.hdf5\n",
            "Epoch 17/100\n",
            "134108/134108 [==============================] - 46s 340us/step - loss: 1.6917\n",
            "\n",
            "Epoch 00017: loss improved from 1.70766 to 1.69167, saving model to lstm.hdf5\n",
            "Epoch 18/100\n",
            "134108/134108 [==============================] - 45s 337us/step - loss: 1.6741\n",
            "\n",
            "Epoch 00018: loss improved from 1.69167 to 1.67409, saving model to lstm.hdf5\n",
            "Epoch 19/100\n",
            "134108/134108 [==============================] - 45s 333us/step - loss: 1.6579\n",
            "\n",
            "Epoch 00019: loss improved from 1.67409 to 1.65795, saving model to lstm.hdf5\n",
            "Epoch 20/100\n",
            "134108/134108 [==============================] - 44s 332us/step - loss: 1.6436\n",
            "\n",
            "Epoch 00020: loss improved from 1.65795 to 1.64363, saving model to lstm.hdf5\n",
            "Epoch 21/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.6289\n",
            "\n",
            "Epoch 00021: loss improved from 1.64363 to 1.62893, saving model to lstm.hdf5\n",
            "Epoch 22/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.6141\n",
            "\n",
            "Epoch 00022: loss improved from 1.62893 to 1.61405, saving model to lstm.hdf5\n",
            "Epoch 23/100\n",
            "134108/134108 [==============================] - 44s 332us/step - loss: 1.6031\n",
            "\n",
            "Epoch 00023: loss improved from 1.61405 to 1.60313, saving model to lstm.hdf5\n",
            "Epoch 24/100\n",
            "134108/134108 [==============================] - 45s 335us/step - loss: 1.5948\n",
            "\n",
            "Epoch 00024: loss improved from 1.60313 to 1.59479, saving model to lstm.hdf5\n",
            "Epoch 25/100\n",
            "134108/134108 [==============================] - 45s 333us/step - loss: 1.5841\n",
            "\n",
            "Epoch 00025: loss improved from 1.59479 to 1.58413, saving model to lstm.hdf5\n",
            "Epoch 26/100\n",
            "134108/134108 [==============================] - 45s 332us/step - loss: 1.5704\n",
            "\n",
            "Epoch 00026: loss improved from 1.58413 to 1.57043, saving model to lstm.hdf5\n",
            "Epoch 27/100\n",
            "134108/134108 [==============================] - 44s 332us/step - loss: 1.5626\n",
            "\n",
            "Epoch 00027: loss improved from 1.57043 to 1.56263, saving model to lstm.hdf5\n",
            "Epoch 28/100\n",
            "134108/134108 [==============================] - 44s 330us/step - loss: 1.5510\n",
            "\n",
            "Epoch 00028: loss improved from 1.56263 to 1.55102, saving model to lstm.hdf5\n",
            "Epoch 29/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.5443\n",
            "\n",
            "Epoch 00029: loss improved from 1.55102 to 1.54429, saving model to lstm.hdf5\n",
            "Epoch 30/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.5352\n",
            "\n",
            "Epoch 00030: loss improved from 1.54429 to 1.53522, saving model to lstm.hdf5\n",
            "Epoch 31/100\n",
            "134108/134108 [==============================] - 45s 334us/step - loss: 1.5294\n",
            "\n",
            "Epoch 00031: loss improved from 1.53522 to 1.52942, saving model to lstm.hdf5\n",
            "Epoch 32/100\n",
            "134108/134108 [==============================] - 45s 332us/step - loss: 1.5167\n",
            "\n",
            "Epoch 00032: loss improved from 1.52942 to 1.51669, saving model to lstm.hdf5\n",
            "Epoch 33/100\n",
            "134108/134108 [==============================] - 45s 332us/step - loss: 1.5142\n",
            "\n",
            "Epoch 00033: loss improved from 1.51669 to 1.51422, saving model to lstm.hdf5\n",
            "Epoch 34/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.5065\n",
            "\n",
            "Epoch 00034: loss improved from 1.51422 to 1.50645, saving model to lstm.hdf5\n",
            "Epoch 35/100\n",
            "134108/134108 [==============================] - 44s 330us/step - loss: 1.4951\n",
            "\n",
            "Epoch 00035: loss improved from 1.50645 to 1.49508, saving model to lstm.hdf5\n",
            "Epoch 36/100\n",
            "134108/134108 [==============================] - 44s 332us/step - loss: 1.4891\n",
            "\n",
            "Epoch 00036: loss improved from 1.49508 to 1.48907, saving model to lstm.hdf5\n",
            "Epoch 37/100\n",
            "134108/134108 [==============================] - 44s 332us/step - loss: 1.4836\n",
            "\n",
            "Epoch 00037: loss improved from 1.48907 to 1.48364, saving model to lstm.hdf5\n",
            "Epoch 38/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.4798\n",
            "\n",
            "Epoch 00038: loss improved from 1.48364 to 1.47982, saving model to lstm.hdf5\n",
            "Epoch 39/100\n",
            "134108/134108 [==============================] - 44s 328us/step - loss: 1.4689\n",
            "\n",
            "Epoch 00039: loss improved from 1.47982 to 1.46893, saving model to lstm.hdf5\n",
            "Epoch 40/100\n",
            "134108/134108 [==============================] - 44s 328us/step - loss: 1.4635\n",
            "\n",
            "Epoch 00040: loss improved from 1.46893 to 1.46354, saving model to lstm.hdf5\n",
            "Epoch 41/100\n",
            "134108/134108 [==============================] - 44s 327us/step - loss: 1.4572\n",
            "\n",
            "Epoch 00041: loss improved from 1.46354 to 1.45723, saving model to lstm.hdf5\n",
            "Epoch 42/100\n",
            "134108/134108 [==============================] - 44s 327us/step - loss: 1.4546\n",
            "\n",
            "Epoch 00042: loss improved from 1.45723 to 1.45463, saving model to lstm.hdf5\n",
            "Epoch 43/100\n",
            "134108/134108 [==============================] - 44s 327us/step - loss: 1.4483\n",
            "\n",
            "Epoch 00043: loss improved from 1.45463 to 1.44833, saving model to lstm.hdf5\n",
            "Epoch 44/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.4389\n",
            "\n",
            "Epoch 00044: loss improved from 1.44833 to 1.43886, saving model to lstm.hdf5\n",
            "Epoch 45/100\n",
            "134108/134108 [==============================] - 45s 334us/step - loss: 1.4388\n",
            "\n",
            "Epoch 00045: loss improved from 1.43886 to 1.43884, saving model to lstm.hdf5\n",
            "Epoch 46/100\n",
            "134108/134108 [==============================] - 44s 332us/step - loss: 1.4336\n",
            "\n",
            "Epoch 00046: loss improved from 1.43884 to 1.43359, saving model to lstm.hdf5\n",
            "Epoch 47/100\n",
            "134108/134108 [==============================] - 44s 332us/step - loss: 1.4296\n",
            "\n",
            "Epoch 00047: loss improved from 1.43359 to 1.42963, saving model to lstm.hdf5\n",
            "Epoch 48/100\n",
            "134108/134108 [==============================] - 45s 332us/step - loss: 1.4206\n",
            "\n",
            "Epoch 00048: loss improved from 1.42963 to 1.42057, saving model to lstm.hdf5\n",
            "Epoch 49/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.4215\n",
            "\n",
            "Epoch 00049: loss did not improve from 1.42057\n",
            "Epoch 50/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.4155\n",
            "\n",
            "Epoch 00050: loss improved from 1.42057 to 1.41553, saving model to lstm.hdf5\n",
            "Epoch 51/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.4050\n",
            "\n",
            "Epoch 00051: loss improved from 1.41553 to 1.40499, saving model to lstm.hdf5\n",
            "Epoch 52/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.4063\n",
            "\n",
            "Epoch 00052: loss did not improve from 1.40499\n",
            "Epoch 53/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.4004\n",
            "\n",
            "Epoch 00053: loss improved from 1.40499 to 1.40037, saving model to lstm.hdf5\n",
            "Epoch 54/100\n",
            "134108/134108 [==============================] - 44s 330us/step - loss: 1.3997\n",
            "\n",
            "Epoch 00054: loss improved from 1.40037 to 1.39967, saving model to lstm.hdf5\n",
            "Epoch 55/100\n",
            "134108/134108 [==============================] - 44s 330us/step - loss: 1.3894\n",
            "\n",
            "Epoch 00055: loss improved from 1.39967 to 1.38942, saving model to lstm.hdf5\n",
            "Epoch 56/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.3852\n",
            "\n",
            "Epoch 00056: loss improved from 1.38942 to 1.38519, saving model to lstm.hdf5\n",
            "Epoch 57/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.3882\n",
            "\n",
            "Epoch 00057: loss did not improve from 1.38519\n",
            "Epoch 58/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.3782\n",
            "\n",
            "Epoch 00058: loss improved from 1.38519 to 1.37821, saving model to lstm.hdf5\n",
            "Epoch 59/100\n",
            "134108/134108 [==============================] - 44s 332us/step - loss: 1.3752\n",
            "\n",
            "Epoch 00059: loss improved from 1.37821 to 1.37523, saving model to lstm.hdf5\n",
            "Epoch 60/100\n",
            "134108/134108 [==============================] - 44s 328us/step - loss: 1.3732\n",
            "\n",
            "Epoch 00060: loss improved from 1.37523 to 1.37320, saving model to lstm.hdf5\n",
            "Epoch 61/100\n",
            "134108/134108 [==============================] - 44s 327us/step - loss: 1.3718\n",
            "\n",
            "Epoch 00061: loss improved from 1.37320 to 1.37183, saving model to lstm.hdf5\n",
            "Epoch 62/100\n",
            "134108/134108 [==============================] - 44s 330us/step - loss: 1.3646\n",
            "\n",
            "Epoch 00062: loss improved from 1.37183 to 1.36456, saving model to lstm.hdf5\n",
            "Epoch 63/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.3661\n",
            "\n",
            "Epoch 00063: loss did not improve from 1.36456\n",
            "Epoch 64/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.3581\n",
            "\n",
            "Epoch 00064: loss improved from 1.36456 to 1.35809, saving model to lstm.hdf5\n",
            "Epoch 65/100\n",
            "134108/134108 [==============================] - 44s 328us/step - loss: 1.3565\n",
            "\n",
            "Epoch 00065: loss improved from 1.35809 to 1.35649, saving model to lstm.hdf5\n",
            "Epoch 66/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.3487\n",
            "\n",
            "Epoch 00066: loss improved from 1.35649 to 1.34865, saving model to lstm.hdf5\n",
            "Epoch 67/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.3514\n",
            "\n",
            "Epoch 00067: loss did not improve from 1.34865\n",
            "Epoch 68/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.3453\n",
            "\n",
            "Epoch 00068: loss improved from 1.34865 to 1.34531, saving model to lstm.hdf5\n",
            "Epoch 69/100\n",
            "134108/134108 [==============================] - 44s 330us/step - loss: 1.3434\n",
            "\n",
            "Epoch 00069: loss improved from 1.34531 to 1.34337, saving model to lstm.hdf5\n",
            "Epoch 70/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.3417\n",
            "\n",
            "Epoch 00070: loss improved from 1.34337 to 1.34166, saving model to lstm.hdf5\n",
            "Epoch 71/100\n",
            "134108/134108 [==============================] - 45s 332us/step - loss: 1.3319\n",
            "\n",
            "Epoch 00071: loss improved from 1.34166 to 1.33194, saving model to lstm.hdf5\n",
            "Epoch 72/100\n",
            "134108/134108 [==============================] - 45s 332us/step - loss: 1.3315\n",
            "\n",
            "Epoch 00072: loss improved from 1.33194 to 1.33151, saving model to lstm.hdf5\n",
            "Epoch 73/100\n",
            "134108/134108 [==============================] - 45s 335us/step - loss: 1.3328\n",
            "\n",
            "Epoch 00073: loss did not improve from 1.33151\n",
            "Epoch 74/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.3253\n",
            "\n",
            "Epoch 00074: loss improved from 1.33151 to 1.32529, saving model to lstm.hdf5\n",
            "Epoch 75/100\n",
            "134108/134108 [==============================] - 44s 330us/step - loss: 1.3241\n",
            "\n",
            "Epoch 00075: loss improved from 1.32529 to 1.32411, saving model to lstm.hdf5\n",
            "Epoch 76/100\n",
            "134108/134108 [==============================] - 44s 330us/step - loss: 1.3167\n",
            "\n",
            "Epoch 00076: loss improved from 1.32411 to 1.31674, saving model to lstm.hdf5\n",
            "Epoch 77/100\n",
            "134108/134108 [==============================] - 44s 327us/step - loss: 1.3189\n",
            "\n",
            "Epoch 00077: loss did not improve from 1.31674\n",
            "Epoch 78/100\n",
            "134108/134108 [==============================] - 44s 327us/step - loss: 1.3162\n",
            "\n",
            "Epoch 00078: loss improved from 1.31674 to 1.31618, saving model to lstm.hdf5\n",
            "Epoch 79/100\n",
            "134108/134108 [==============================] - 44s 328us/step - loss: 1.3175\n",
            "\n",
            "Epoch 00079: loss did not improve from 1.31618\n",
            "Epoch 80/100\n",
            "134108/134108 [==============================] - 44s 331us/step - loss: 1.3080\n",
            "\n",
            "Epoch 00080: loss improved from 1.31618 to 1.30797, saving model to lstm.hdf5\n",
            "Epoch 81/100\n",
            "134108/134108 [==============================] - 44s 326us/step - loss: 1.3051\n",
            "\n",
            "Epoch 00081: loss improved from 1.30797 to 1.30511, saving model to lstm.hdf5\n",
            "Epoch 82/100\n",
            "134108/134108 [==============================] - 44s 325us/step - loss: 1.3067\n",
            "\n",
            "Epoch 00082: loss did not improve from 1.30511\n",
            "Epoch 83/100\n",
            "134108/134108 [==============================] - 44s 329us/step - loss: 1.3040\n",
            "\n",
            "Epoch 00083: loss improved from 1.30511 to 1.30399, saving model to lstm.hdf5\n",
            "Epoch 84/100\n",
            "134108/134108 [==============================] - 50s 370us/step - loss: 1.2998\n",
            "\n",
            "Epoch 00084: loss improved from 1.30399 to 1.29979, saving model to lstm.hdf5\n",
            "Epoch 85/100\n",
            "134108/134108 [==============================] - 45s 338us/step - loss: 1.3048\n",
            "\n",
            "Epoch 00085: loss did not improve from 1.29979\n",
            "Epoch 86/100\n",
            "134108/134108 [==============================] - 46s 340us/step - loss: 1.2957\n",
            "\n",
            "Epoch 00086: loss improved from 1.29979 to 1.29568, saving model to lstm.hdf5\n",
            "Epoch 87/100\n",
            "134108/134108 [==============================] - 46s 344us/step - loss: 1.2913\n",
            "\n",
            "Epoch 00087: loss improved from 1.29568 to 1.29131, saving model to lstm.hdf5\n",
            "Epoch 88/100\n",
            "134108/134108 [==============================] - 45s 339us/step - loss: 1.2910\n",
            "\n",
            "Epoch 00088: loss improved from 1.29131 to 1.29104, saving model to lstm.hdf5\n",
            "Epoch 89/100\n",
            "134108/134108 [==============================] - 46s 339us/step - loss: 1.2906\n",
            "\n",
            "Epoch 00089: loss improved from 1.29104 to 1.29062, saving model to lstm.hdf5\n",
            "Epoch 90/100\n",
            "134108/134108 [==============================] - 45s 339us/step - loss: 1.2851\n",
            "\n",
            "Epoch 00090: loss improved from 1.29062 to 1.28514, saving model to lstm.hdf5\n",
            "Epoch 91/100\n",
            "134108/134108 [==============================] - 46s 339us/step - loss: 1.2853\n",
            "\n",
            "Epoch 00091: loss did not improve from 1.28514\n",
            "Epoch 92/100\n",
            "134108/134108 [==============================] - 45s 339us/step - loss: 1.2804\n",
            "\n",
            "Epoch 00092: loss improved from 1.28514 to 1.28045, saving model to lstm.hdf5\n",
            "Epoch 93/100\n",
            "134108/134108 [==============================] - 46s 341us/step - loss: 1.2793\n",
            "\n",
            "Epoch 00093: loss improved from 1.28045 to 1.27930, saving model to lstm.hdf5\n",
            "Epoch 94/100\n",
            "134108/134108 [==============================] - 45s 339us/step - loss: 1.2759\n",
            "\n",
            "Epoch 00094: loss improved from 1.27930 to 1.27589, saving model to lstm.hdf5\n",
            "Epoch 95/100\n",
            "134108/134108 [==============================] - 45s 339us/step - loss: 1.2740\n",
            "\n",
            "Epoch 00095: loss improved from 1.27589 to 1.27403, saving model to lstm.hdf5\n",
            "Epoch 96/100\n",
            "134108/134108 [==============================] - 45s 338us/step - loss: 1.2708\n",
            "\n",
            "Epoch 00096: loss improved from 1.27403 to 1.27082, saving model to lstm.hdf5\n",
            "Epoch 97/100\n",
            "134108/134108 [==============================] - 45s 339us/step - loss: 1.2628\n",
            "\n",
            "Epoch 00097: loss improved from 1.27082 to 1.26281, saving model to lstm.hdf5\n",
            "Epoch 98/100\n",
            "134108/134108 [==============================] - 45s 339us/step - loss: 1.2676\n",
            "\n",
            "Epoch 00098: loss did not improve from 1.26281\n",
            "Epoch 99/100\n",
            "134108/134108 [==============================] - 45s 337us/step - loss: 1.2641\n",
            "\n",
            "Epoch 00099: loss did not improve from 1.26281\n",
            "Epoch 100/100\n",
            "134108/134108 [==============================] - 46s 342us/step - loss: 1.2587\n",
            "\n",
            "Epoch 00100: loss improved from 1.26281 to 1.25870, saving model to lstm.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f45f65e0a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACxOFBv9jyPv",
        "colab_type": "text"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boBkWDHGFrS2",
        "colab_type": "code",
        "outputId": "a7fb4dac-0aee-47e2-c28f-67eaceed96e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# filename = \"weights-improvement-20-1.9415.hdf5\"\n",
        "\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start].tolist()\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "final_result=''\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  final_result=final_result+result\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "print(final_result)  \n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"  and among\n",
            "them \"\n",
            "ed tuitted tetlaie tatting nike the leet tarping aboutse io tepimbny seruon moog anice whatsedl goce iere anice aspearddy iom yery hoisge woder teadatdie taie anice aso asd teeaye hettiny seraies anice anl ie ganlet teparked tatticl asoear anl tound anl tuoaled oear oose anice whisg waie tatting as her oex uhe hot uoeeziog anl abouune oear tuote iere anice aspearddy iom yery hoisge woder teadatdie taie anice aso asd teeaye hettiny seraies anice anl ie ganlet teparked tatticl asoear anl tound anl\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gnZVlQWyuLq",
        "colab_type": "code",
        "outputId": "93b47f59-4f0a-4439-afee-7248af4b55a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# filename = \"weights-improvement-20-1.9415.hdf5\"\n",
        "\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start].tolist()\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "final_result=''\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  final_result=final_result+result\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "print(final_result)  \n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" th\n",
            "their heads  \"\n",
            "oext asd teadh anioe tetlars wher suested anl toayed iir veart ooeered teeoiny anice anice whoygh oar exery goowr hoerse tame anice aspearddy iom yery hoisge woder teadatdie taie anice aso asd teeaye hettiny seraies anice anl ie ganlet teparked tatticl asoear anl tound anl tuoaled oear oose anice whisg waie tatting as her oex uhe hot uoeeziog anl abouune oear tuote iere anice aspearddy iom yery hoisge woder teadatdie taie anice aso asd teeaye hettiny seraies anice anl ie ganlet teparked tatticl \n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}